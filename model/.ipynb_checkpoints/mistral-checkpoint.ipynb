{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54ac426",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "\n",
    "!pip install -qqq pyautogen modelz-llm huggingface_hub \n",
    "!pip install -q datasets loralib sentencepiece\n",
    "!pip -qqq install xformers einops\n",
    "!pip -qqq install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48e52a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Using cached scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /cluster/home/danlin/vit/lib/python3.10/site-packages (from scipy) (1.26.0)\n",
      "Using cached scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.11.4\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88298685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71ef613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:16<00:00,  8.47s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "from transformers import GenerationConfig, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        load_in_4bit=False,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118c01a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bd703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{\"role\": \"user\", \"content\": \"Tell me about the CERG institute at NTNU.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "886eaef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Tell me about the CERG institute at NTNU. [/INST] The CERG institute at NTNU, also known as the Norwegian Center for Excellence in Research in Gaming, is a center that focuses on research and education in the field of gaming. It was established in 2007 and is considered one of the leading gaming research institutions in the world. The institute is part of the Norwegian Academy of Science and Technology (NTNU) and operates under the Faculty of Humanities.\n",
      "\n",
      "The mission of CERG is to conduct high-quality research and education in gaming and to contribute to the development of the gaming industry. The institute aims to advance our understanding of gaming and its social, cultural, and economic impacts, and to use this knowledge to improve the design and development of games.\n",
      "\n",
      "CERG has a team of expert researchers from a variety of fields, including computer science, psychology, sociology, and game studies. The institute is also home to several research groups, including the Game Studies and Design Research Group, the Social and Cultural Aspects of Gaming Research Group, and the Games and Learning Research Group.\n",
      "\n",
      "The institute has a strong focus on innovation and collaboration, with partnerships with leading gaming companies, other research institutions, and industry associations. CERG also hosts workshops, seminars, and conferences to promote knowledge exchange and collaboration within the gaming community.\n",
      "\n",
      "Overall, CERG is a dynamic and engaged research center that plays a vital role in shaping the future of the gaming industry and advancing our understanding of this rapidly growing field.</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about the CERG institute at NTNU.\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cba4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit_kernel",
   "language": "python",
   "name": "vit_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
